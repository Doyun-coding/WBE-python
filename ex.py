import webrtcvad
import collections
import sounddevice as sd
import numpy as np
import whisper
import os
import tempfile
import scipy.io.wavfile as wav
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# OpenAI api key
openai_api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=openai_api_key)
# process ëª¨ë¸ ì„¤ì •
model = whisper.load_model("base")

# ìŒì„± ì¸ì‹ ë¯¼ê°ë„ 2ë¡œ ì„¤ì • / 0~3 (3ì´ ê°€ì¥ ë¯¼ê°)
vad = webrtcvad.Vad(2)

# ë…¹ìŒ íŒŒë¼ë¯¸í„°
sample_rate = 16000     # 1ì´ˆì— 16000ê°œì˜ ì˜¤ë””ì˜¤ ìƒ˜í”Œì„ ë…¹ìŒ
frame_duration = 30     # ì˜¤ë””ì˜¤ë¥¼ 30ms ë‹¨ìœ„ë¡œ ì˜ë¼ì„œ ë¶„ì„
frame_size = int(sample_rate * frame_duration / 1000)   # 30ms ë™ì•ˆì˜ ì˜¤ë””ì˜¤ë¥¼ ìƒ˜í”Œë§í•˜ë©´ ì´ 480ê°œì˜ ìƒ˜í”Œì´ ìƒê¸´ë‹¤ëŠ” ì˜ë¯¸
channels = 1            # ëª¨ë…¸ ì±„ë„
silence_threshold = 33  # ë¬´ìŒì´ 15í”„ë ˆì„ ì—°ì†(ì•½ 30 Ã— 30ms = 1ì´ˆ) ê°ì§€ë˜ë©´ "ë…¹ìŒ ëë‚¬ë‹¤"ê³  íŒë‹¨


def is_speech(frame_bytes):
    return vad.is_speech(frame_bytes, sample_rate)

# ì‚¬ìš©ìê°€ ë§í•˜ê¸° ì‹œì‘í•˜ë©´ ìë™ ë…¹ìŒ ì‹œì‘, ì¹¨ë¬µì´ ì´ì–´ì§€ë©´ ì¢…ë£Œ
def record_triggered_by_voice():
    print("ğŸŸ¢ ëŒ€ê¸° ì¤‘... (ë§í•˜ë©´ ìë™ ë…¹ìŒ ì‹œì‘)")

    ring_buffer = collections.deque(maxlen=silence_threshold)
    recording = []
    triggered = False
    silence_count = 0

    stop_flag = {"stop": False}  # stopì„ ì™¸ë¶€ì—ì„œ ì œì–´í•˜ê¸° ìœ„í•œ í”Œë˜ê·¸

    def callback(indata, frames, time, status):
        nonlocal triggered, silence_count, recording
        pcm = indata[:, 0]
        pcm_bytes = (pcm * 32768).astype(np.int16).tobytes()
        speech = is_speech(pcm_bytes)
        volume = np.max(np.abs(pcm))

        if speech and volume > 0.02:
            print("ğŸ™ï¸ ìŒì„± ê°ì§€ ì¤‘...")
        else:
            print("ğŸ”ˆ ë¬´ìŒ ìƒíƒœ...")

        if triggered:
            recording.append(pcm.copy())
            if not speech:
                silence_count += 1
                if silence_count > silence_threshold:
                    print("ğŸ”‡ ë¬´ìŒ ê°ì§€ â†’ ë…¹ìŒ ì¢…ë£Œ")
                    stop_flag["stop"] = True  # í”Œë˜ê·¸ë¡œ ì¢…ë£Œ ì‹ í˜¸
            else:
                silence_count = 0
        else:
            ring_buffer.append(pcm.copy())
            if speech and volume > 0.02:
                print("ğŸ¤ ìŒì„± ì‹œì‘ â†’ ë…¹ìŒ ì‹œì‘")
                triggered = True
                recording.extend(ring_buffer)
                ring_buffer.clear()

    # ìˆ˜ë™ ìŠ¤íŠ¸ë¦¼ ì œì–´ ë°©ì‹
    stream = sd.InputStream(channels=channels, samplerate=sample_rate,
                            blocksize=frame_size, dtype='float32',
                            callback=callback)
    with stream:
        while not stop_flag["stop"]:
            sd.sleep(50)  # 50ms ê°„ê²©ìœ¼ë¡œ stop ìƒíƒœ í™•ì¸

    print("ğŸ›‘ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ")

    return np.concatenate(recording)

def main():
    while True:

        # ìë™ ë…¹ìŒ
        audio_data = record_triggered_by_voice()

        if len(audio_data) == 0:
            print("âš ï¸ ë…¹ìŒëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ëŒ€ê¸°í•©ë‹ˆë‹¤.")
            continue

        print("ğŸŸ¢ ì™„ë£Œ... (ë…¹ìŒ ì™„ë£Œ)")

        # ì„ì‹œ wav íŒŒì¼ ì €ì¥
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmpfile:
            wav.write(tmpfile.name, sample_rate, (audio_data * 32768).astype(np.int16))
            temp_filename = tmpfile.name

        print(f"ğŸ“ ì €ì¥ëœ WAV íŒŒì¼: {temp_filename} ({os.path.getsize(temp_filename)} bytes)")

        # Whisper ì²˜ë¦¬
        result = model.transcribe(temp_filename)
        raw_text = result["text"]
        print("ğŸ§ Whisper:", raw_text)

        # GPT-4 ì •ì œ
        prompt = f"""
        ë„ˆëŠ” ë¦¬ê·¸ ì˜¤ë¸Œ ë ˆì „ë“œ(LOL) ê²Œì„ì˜ ì‹¤ì‹œê°„ ë³´ì´ìŠ¤ ì±„íŒ…ì„ í…ìŠ¤íŠ¸ë¡œ ë°”ê¿”ì£¼ëŠ” AIì•¼.
        ë‹¤ìŒ ë¬¸ì¥ì€ ìŒì„± ì¸ì‹ ê²°ê³¼ì¸ë°, ì´ë¥¼ **LOL ê²Œì„ ë‚´ ì±„íŒ… ìŠ¤íƒ€ì¼**ì— ë§ê²Œ **ê°„ê²°í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì •ì œ**í•´ì¤˜.
        ê²Œì„ ë‚´ ìš©ì–´ ì˜ë¯¸ ì „ë‹¬ì´ í™•ì‹¤í•˜ê²Œ ë˜ë„ë¡ í•´ì¤˜.
        í•œê¸€ë¡œ ëŒ€ë‹µí•´ì¤˜
        ì…ë ¥: "{raw_text}"
        ì¶œë ¥:
        """
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” ë¦¬ê·¸ ì˜¤ë¸Œ ë ˆì „ë“œ(LOL) ê²Œì„ì˜ ì‹¤ì‹œê°„ ë³´ì´ìŠ¤ ì±„íŒ…ì„ í…ìŠ¤íŠ¸ë¡œ ë°”ê¿”ì£¼ëŠ” AIì•¼."
                                              "ë‹¤ìŒ ë¬¸ì¥ì€ ìŒì„± ì¸ì‹ ê²°ê³¼ì¸ë°, ì´ë¥¼ **LOL ê²Œì„ ë‚´ ì±„íŒ… ìŠ¤íƒ€ì¼**ì— ë§ê²Œ **ê°„ê²°í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì •ì œ**í•´ì¤˜."
                                              "ê²Œì„ ë‚´ ìš©ì–´ ì˜ë¯¸ ì „ë‹¬ì´ í™•ì‹¤í•˜ê²Œ ë˜ë„ë¡ í•´ì¤˜. í•œê¸€ë¡œ ëŒ€ë‹µí•´ì¤˜"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=100
        )
        final_text = response.choices[0].message.content.strip()
        print("âœ¨ ì •ì œ:", final_text)

        # TTS
        tts = client.audio.speech.create(model="tts-1", voice="nova", input=final_text)
        with open("mp3/output.mp3", "wb") as f:
            f.write(tts.content)
        os.system("afplay output.mp3")

        os.remove(temp_filename)


if __name__ == "__main__":
    main()
